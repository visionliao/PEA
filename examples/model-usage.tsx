/**
 * å¤§æ¨¡å‹æ¶æ„ä½¿ç”¨ç¤ºä¾‹
 * è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ modelService è¿›è¡Œå„ç§æ¨¡å‹è°ƒç”¨
 */

import { modelService } from '@/lib/model-service'
import { LLMRequest, LLMResponse } from '@/types/llm'
import { createMessages } from '@/lib/type-utils'
import { useState } from 'react'

// =============================================================================
// 1. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
// =============================================================================

/**
 * åˆå§‹åŒ–æ¨¡å‹æœåŠ¡
 * åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡
 */
export async function initializeExample() {
  try {
    await modelService.initialize()
    console.log('âœ… æ¨¡å‹æœåŠ¡åˆå§‹åŒ–æˆåŠŸ')
  } catch (error) {
    console.error('âŒ æ¨¡å‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥:', error)
  }
}

/**
 * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export async function getAvailableModelsExample() {
  const availableModels = modelService.getAvailableModels()
  console.log('ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:')
  availableModels.forEach(model => {
    console.log(`  - ${model.name} (${model.id}) - ${model.provider}`)
  })
  return availableModels
}

/**
 * åŸºç¡€æ¨¡å‹è°ƒç”¨ç¤ºä¾‹
 */
export async function basicCallExample() {
  const request: LLMRequest = {
    messages: createMessages.simple(
      'è¯·ç”¨ç®€çŸ­çš„è¯­è¨€ä»‹ç»ä¸€ä¸‹Reactçš„ä¸»è¦ç‰¹ç‚¹ã€‚',
      'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚'
    ),
    params: {
      temperature: 0.7,
      maxTokens: 500
    }
  }

  try {
    const result = await modelService.call({
      modelId: 'gpt-4o',
      request,
      onComplete: (response: LLMResponse) => {
        console.log('âœ… è°ƒç”¨å®Œæˆ:', response.content)
      }
    })

    if (result.success) {
      console.log('ğŸ‰ è°ƒç”¨æˆåŠŸ!')
      console.log('ğŸ’¬ å›å¤å†…å®¹:', result.response?.content)
      console.log('ğŸ“Š ä½¿ç”¨æƒ…å†µ:', result.usage)
      console.log('â±ï¸ è€—æ—¶:', result.duration, 'ms')
    } else {
      console.error('âŒ è°ƒç”¨å¤±è´¥:', result.error?.message)
    }
    
    return result
  } catch (error) {
    console.error('âŒ è°ƒç”¨å¼‚å¸¸:', error)
    throw error
  }
}

// =============================================================================
// 2. é«˜çº§åŠŸèƒ½ç¤ºä¾‹
// =============================================================================

/**
 * æµå¼è°ƒç”¨ç¤ºä¾‹ - å®æ—¶æ˜¾ç¤ºå›å¤å†…å®¹
 */
export async function streamCallExample() {
  const request: LLMRequest = {
    messages: createMessages.simple('è¯·è®²ä¸€ä¸ªç®€çŸ­çš„ç§‘æŠ€æ•…äº‹')
  }

  let fullContent = ''
  
  try {
    const result = await modelService.call({
      modelId: 'gpt-4o',
      request,
      stream: true,
      onProgress: (chunk) => {
        if (chunk.content) {
          fullContent += chunk.content
          // å®æ—¶è¾“å‡ºå†…å®¹ (åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥æ›´æ–°UI)
          process.stdout.write(chunk.content)
        }
      },
      onComplete: (response) => {
        console.log('\nâœ… æµå¼è°ƒç”¨å®Œæˆ!')
      }
    })

    console.log('\nğŸ“ å®Œæ•´å†…å®¹:', fullContent)
    return result
  } catch (error) {
    console.error('âŒ æµå¼è°ƒç”¨å¤±è´¥:', error)
    throw error
  }
}

/**
 * æ‰¹é‡è°ƒç”¨ç¤ºä¾‹ - åŒæ—¶è°ƒç”¨å¤šä¸ªæ¨¡å‹æˆ–å‘é€å¤šä¸ªè¯·æ±‚
 */
export async function batchCallExample() {
  const requests = [
    {
      modelId: 'gpt-4o',
      request: { 
        messages: createMessages.simple('ç”¨ä¸€å¥è¯ä»‹ç»JavaScript') 
      }
    },
    {
      modelId: 'gemini-2.5-flash',
      request: { 
        messages: createMessages.simple('ç”¨ä¸€å¥è¯ä»‹ç»Python') 
      }
    },
    {
      modelId: 'gpt-4o-mini',
      request: { 
        messages: createMessages.simple('ç”¨ä¸€å¥è¯ä»‹ç»TypeScript') 
      }
    }
  ]

  try {
    console.log('ğŸš€ å¼€å§‹æ‰¹é‡è°ƒç”¨...')
    const batchResults = await modelService.batchCall(requests)
    
    console.log('ğŸ“‹ æ‰¹é‡è°ƒç”¨ç»“æœ:')
    batchResults.forEach((result, index) => {
      const status = result.success ? 'âœ…' : 'âŒ'
      const modelId = requests[index].modelId
      console.log(`${status} ${modelId}:`, 
        result.success ? result.response?.content : result.error?.message
      )
    })
    
    return batchResults
  } catch (error) {
    console.error('âŒ æ‰¹é‡è°ƒç”¨å¤±è´¥:', error)
    throw error
  }
}

/**
 * æ¨¡å‹æ¯”è¾ƒç¤ºä¾‹ - ç”¨åŒä¸€ä¸ªpromptæµ‹è¯•ä¸åŒæ¨¡å‹
 */
export async function modelComparisonExample() {
  const prompt = 'è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ã€‚'
  const modelIds = ['gpt-4o', 'gemini-2.5-flash', 'gpt-4o-mini']

  try {
    console.log('ğŸ” å¼€å§‹æ¨¡å‹æ¯”è¾ƒ...')
    const comparison = await modelService.compareModels(modelIds, prompt)
    
    console.log('ğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:')
    comparison.forEach(({ modelId, result }) => {
      const status = result.success ? 'âœ…' : 'âŒ'
      const duration = result.duration
      console.log(`\n${status} ${modelId} (${duration}ms):`)
      if (result.success) {
        console.log(`  ğŸ’¬ ${result.response?.content?.substring(0, 100)}...`)
        if (result.usage) {
          console.log(`  ğŸ“Š Tokens: ${result.usage.totalTokens}`)
        }
      } else {
        console.log(`  âŒ ${result.error?.message}`)
      }
    })
    
    return comparison
  } catch (error) {
    console.error('âŒ æ¨¡å‹æ¯”è¾ƒå¤±è´¥:', error)
    throw error
  }
}

// =============================================================================
// 3. å®ç”¨åŠŸèƒ½ç¤ºä¾‹
// =============================================================================

/**
 * è·å–ä»»åŠ¡æ¨èæ¨¡å‹
 */
export function getRecommendedModelsExample() {
  const tasks = ['coding', 'writing', 'analysis', 'chat', 'translation']
  
  tasks.forEach(task => {
    const recommended = modelService.getRecommendedModels(task)
    console.log(`ğŸ¯ ${task} ä»»åŠ¡æ¨èæ¨¡å‹:`, recommended.join(', '))
  })
}

/**
 * æ¨¡å‹é…ç½®éªŒè¯
 */
export function validateModelConfigExample() {
  const modelIds = ['gpt-4o', 'gemini-2.5-flash', 'invalid-model']
  
  modelIds.forEach(modelId => {
    const validation = modelService.validateModel(modelId)
    const status = validation.isValid ? 'âœ…' : 'âŒ'
    console.log(`${status} ${modelId}:`, 
      validation.isValid ? 'é…ç½®æœ‰æ•ˆ' : validation.errors.join(', ')
    )
  })
}

/**
 * æ¨¡å‹æµ‹è¯• - å¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
 */
export async function testModelExample() {
  const modelIds = ['gpt-4o', 'gemini-2.5-flash']
  
  for (const modelId of modelIds) {
    try {
      console.log(`ğŸ§ª æµ‹è¯•æ¨¡å‹: ${modelId}`)
      const result = await modelService.testModel(modelId)
      
      if (result.success) {
        console.log(`âœ… ${modelId} æµ‹è¯•æˆåŠŸ`)
        console.log(`  ğŸ’¬ ${result.response?.content?.substring(0, 50)}...`)
      } else {
        console.log(`âŒ ${modelId} æµ‹è¯•å¤±è´¥:`, result.error?.message)
      }
    } catch (error) {
      console.log(`âŒ ${modelId} æµ‹è¯•å¼‚å¸¸:`, error)
    }
  }
}

// =============================================================================
// 4. React ç»„ä»¶ç¤ºä¾‹
// =============================================================================

/**
 * React Hook ç¤ºä¾‹ - åœ¨ç»„ä»¶ä¸­ä½¿ç”¨æ¨¡å‹æœåŠ¡
 */
export function useModelCall() {
  const [response, setResponse] = useState('')
  const [isLoading, setIsLoading] = useState(false)
  const [error, setError] = useState('')

  const callModel = async (prompt: string, modelId: string = 'gpt-4o') => {
    setIsLoading(true)
    setError('')
    setResponse('')

    try {
      const result = await modelService.call({
        modelId,
        request: {
          messages: createMessages.simple(prompt)
        }
      })
      
      if (result.success && result.response) {
        setResponse(result.response.content)
      } else {
        setError(result.error?.message || 'è°ƒç”¨å¤±è´¥')
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : 'æœªçŸ¥é”™è¯¯')
    } finally {
      setIsLoading(false)
    }
  }

  return { response, isLoading, error, callModel }
}

/**
 * React ç»„ä»¶ç¤ºä¾‹
 */
export function ModelExample() {
  const [prompt, setPrompt] = useState('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±')
  const [selectedModel, setSelectedModel] = useState('gpt-4o')
  const { response, isLoading, error, callModel } = useModelCall()

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault()
    callModel(prompt, selectedModel)
  }

  // è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
  const availableModels = modelService.getAvailableModels()

  return (
    <div className="max-w-2xl mx-auto p-6 space-y-4">
      <h2 className="text-2xl font-bold mb-4">å¤§æ¨¡å‹è°ƒç”¨ç¤ºä¾‹</h2>
      
      <form onSubmit={handleSubmit} className="space-y-4">
        <div>
          <label className="block text-sm font-medium mb-2">é€‰æ‹©æ¨¡å‹</label>
          <select 
            value={selectedModel}
            onChange={(e) => setSelectedModel(e.target.value)}
            className="w-full p-2 border rounded-md"
          >
            {availableModels.map((model) => (
              <option key={model.id} value={model.id}>
                {model.name} ({model.provider})
              </option>
            ))}
          </select>
        </div>
        
        <div>
          <label className="block text-sm font-medium mb-2">è¾“å…¥æç¤ºè¯</label>
          <textarea
            value={prompt}
            onChange={(e) => setPrompt(e.target.value)}
            className="w-full p-2 border rounded-md h-24"
            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."
          />
        </div>
        
        <button
          type="submit"
          disabled={isLoading}
          className="w-full bg-blue-500 text-white py-2 px-4 rounded-md hover:bg-blue-600 disabled:opacity-50"
        >
          {isLoading ? 'è°ƒç”¨ä¸­...' : 'è°ƒç”¨æ¨¡å‹'}
        </button>
      </form>

      {error && (
        <div className="bg-red-50 border border-red-200 rounded-md p-4">
          <h3 className="text-red-800 font-medium">é”™è¯¯</h3>
          <p className="text-red-600 text-sm">{error}</p>
        </div>
      )}

      {response && (
        <div className="bg-green-50 border border-green-200 rounded-md p-4">
          <h3 className="text-green-800 font-medium">æ¨¡å‹å“åº”</h3>
          <p className="text-green-700 whitespace-pre-wrap">{response}</p>
        </div>
      )}
    </div>
  )
}

// =============================================================================
// 5. å®Œæ•´ç¤ºä¾‹ - è¿è¡Œæ‰€æœ‰æ¼”ç¤º
// =============================================================================

/**
 * è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
 */
export async function runAllExamples() {
  console.log('ğŸš€ å¼€å§‹è¿è¡Œå¤§æ¨¡å‹æ¶æ„ç¤ºä¾‹...')
  console.log('=====================================')

  try {
    // 1. åˆå§‹åŒ–
    await initializeExample()
    
    // 2. è·å–æ¨¡å‹åˆ—è¡¨
    await getAvailableModelsExample()
    
    // 3. åŸºç¡€è°ƒç”¨
    await basicCallExample()
    
    // 4. æµå¼è°ƒç”¨
    await streamCallExample()
    
    // 5. æ‰¹é‡è°ƒç”¨
    await batchCallExample()
    
    // 6. æ¨¡å‹æ¯”è¾ƒ
    await modelComparisonExample()
    
    // 7. å®ç”¨åŠŸèƒ½
    getRecommendedModelsExample()
    validateModelConfigExample()
    await testModelExample()
    
    console.log('=====================================')
    console.log('âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!')
    
  } catch (error) {
    console.error('âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥:', error)
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œæ‰€æœ‰ç¤ºä¾‹
if (typeof window === 'undefined') {
  // Node.js ç¯å¢ƒ
  runAllExamples().catch(console.error)
}